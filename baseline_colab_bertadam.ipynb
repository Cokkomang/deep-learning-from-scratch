{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "version": "3.6.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "name": "python",
      "mimetype": "text/x-python"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "premium"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cokkomang/deep-learning-from-scratch/blob/master/baseline_colab_bertadam.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import requirements"
      ],
      "metadata": {
        "id": "TH32rzgprvgM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "G9EvC1HBuf41",
        "outputId": "9aad0908-90ac-4193-9e67-f771c6784cd5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.8/dist-packages (4.25.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.13.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (0.11.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pdb\n",
        "import argparse\n",
        "from dataclasses import dataclass, field\n",
        "from typing import Optional\n",
        "from collections import defaultdict\n",
        "\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "import numpy as np\n",
        "from tqdm import tqdm, trange\n",
        "\n",
        "from transformers import (\n",
        "    BertForSequenceClassification,\n",
        "    BertTokenizer,\n",
        "    AutoConfig,\n",
        "    AdamW\n",
        ")\n"
      ],
      "metadata": {
        "id": "AAdLxrUZrvgP"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "CouR3uGv_S7p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06832ef8-c6f4-464f-847a-51fdeda60ea1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Preprocess"
      ],
      "metadata": {
        "id": "ASWOOmXqrvgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def make_id_file(task, tokenizer):\n",
        "    def make_data_strings(file_name):\n",
        "        data_strings = []\n",
        "        with open(os.path.join(file_name), 'r', encoding='utf-8') as f:\n",
        "            id_file_data = [tokenizer.encode(line.lower()) for line in f.readlines()]\n",
        "        for item in id_file_data:\n",
        "            data_strings.append(' '.join([str(k) for k in item]))\n",
        "        return data_strings\n",
        "    \n",
        "    print('it will take some times...')\n",
        "    train_pos = make_data_strings('/content/drive/MyDrive/sentiment.train.1')\n",
        "    train_neg = make_data_strings('/content/drive/MyDrive/sentiment.train.0')\n",
        "    dev_pos = make_data_strings('/content/drive/MyDrive/sentiment.dev.1')\n",
        "    dev_neg = make_data_strings('/content/drive/MyDrive/sentiment.dev.0')\n",
        "\n",
        "    print('make id file finished!')\n",
        "    return train_pos, train_neg, dev_pos, dev_neg"
      ],
      "metadata": {
        "id": "RAnU6w29rvgR"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "metadata": {
        "id": "Ui2HOCflrvgR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from google.colab import files\n",
        "# uploaded = files.upload()"
      ],
      "metadata": {
        "id": "4jVuK-V1uq3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls"
      ],
      "metadata": {
        "id": "ttzRlY4Ov0jZ",
        "outputId": "d1219467-b17c-4062-be28-5ba4625aba3d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  pytorch_model.bin  sample_data\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos, train_neg, dev_pos, dev_neg = make_id_file('yelp', tokenizer)"
      ],
      "metadata": {
        "id": "BAgztXIBrvgS",
        "outputId": "17bb281f-a38d-4a6e-963c-deb0675417d1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "it will take some times...\n",
            "make id file finished!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_pos[:10]"
      ],
      "metadata": {
        "id": "wRh2WjGRrvgS",
        "outputId": "0a717576-2267-4dc8-d205-472824871790",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['101 6581 2833 1012 102',\n",
              " '101 21688 8013 2326 1012 102',\n",
              " '101 2027 2036 2031 3679 19247 1998 3256 6949 2029 2003 2428 2204 1012 102',\n",
              " '101 2009 1005 1055 1037 2204 15174 2098 7570 22974 2063 1012 102',\n",
              " '101 1996 3095 2003 5379 1012 102',\n",
              " '101 2204 3347 2833 1012 102',\n",
              " '101 2204 2326 1012 102',\n",
              " '101 11350 1997 2154 2003 25628 1998 7167 1997 19247 1012 102',\n",
              " '101 2307 2173 2005 6265 2030 3347 27962 1998 5404 1012 102',\n",
              " '101 1996 2047 2846 3504 6429 1012 102']"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentDataset(object):\n",
        "    def __init__(self, tokenizer, pos, neg):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = []\n",
        "        self.label = []\n",
        "\n",
        "        for pos_sent in pos:\n",
        "            self.data += [self._cast_to_int(pos_sent.strip().split())]\n",
        "            self.label += [[1]]\n",
        "        for neg_sent in neg:\n",
        "            self.data += [self._cast_to_int(neg_sent.strip().split())]\n",
        "            self.label += [[0]]\n",
        "\n",
        "    def _cast_to_int(self, sample):\n",
        "        return [int(word_id) for word_id in sample]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.data[index]\n",
        "        return np.array(sample), np.array(self.label[index])"
      ],
      "metadata": {
        "id": "JdpQQQMUrvgT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = SentimentDataset(tokenizer, train_pos, train_neg)\n",
        "dev_dataset = SentimentDataset(tokenizer, dev_pos, dev_neg)"
      ],
      "metadata": {
        "id": "wCz5ey8xrvgU"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(train_dataset):\n",
        "    print(item)\n",
        "    if i == 10:\n",
        "        break"
      ],
      "metadata": {
        "id": "UuvkMczvrvgU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn_style(samples):\n",
        "    input_ids, labels = zip(*samples)\n",
        "    max_len = max(len(input_id) for input_id in input_ids)\n",
        "    sorted_indices = range(len(input_ids))\n",
        "    \n",
        "    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],\n",
        "                             batch_first=True)\n",
        "    attention_mask = torch.tensor(\n",
        "        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in\n",
        "         sorted_indices])\n",
        "    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])\n",
        "    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])\n",
        "    labels = torch.tensor(np.stack(labels, axis=0)[sorted_indices])\n",
        "\n",
        "    return input_ids, attention_mask, token_type_ids, position_ids, labels"
      ],
      "metadata": {
        "id": "B0wRUBYSrvgU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_batch_size=32 \n",
        "train_batch_size= 256\n",
        "# eval_batch_size=64 \n",
        "eval_batch_size= 512\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset,\n",
        "                                           batch_size=train_batch_size,\n",
        "                                           shuffle=True, collate_fn=collate_fn_style,\n",
        "                                           pin_memory=True, num_workers=2)\n",
        "dev_loader = torch.utils.data.DataLoader(dev_dataset, batch_size=eval_batch_size,\n",
        "                                         shuffle=False, collate_fn=collate_fn_style,\n",
        "                                         num_workers=2)"
      ],
      "metadata": {
        "id": "5saagig0rvgV"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# random seed\n",
        "random_seed=42\n",
        "np.random.seed(random_seed)\n",
        "torch.manual_seed(random_seed)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "zvFqCaCnrvgW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.train()\n",
        "learning_rate = 2e-5\n",
        "#optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "optimizer = AdamW(model.parameters(), lr=learning_rate, correct_bias=False) #BertAdam optimizer"
      ],
      "metadata": {
        "id": "dWwhmyMyrvgW"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_acc(predictions, target_labels):\n",
        "    return (np.array(predictions) == np.array(target_labels)).mean()"
      ],
      "metadata": {
        "id": "MztU-L83rvgW"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train_epoch = 3\n",
        "train_epoch = 1\n",
        "lowest_valid_loss = 9999.\n",
        "for epoch in range(train_epoch):\n",
        "    with tqdm(train_loader, unit=\"batch\") as tepoch:\n",
        "        for iteration, (input_ids, attention_mask, token_type_ids, position_ids, labels) in enumerate(tepoch):\n",
        "            tepoch.set_description(f\"Epoch {epoch}\")\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            token_type_ids = token_type_ids.to(device)\n",
        "            position_ids = position_ids.to(device)\n",
        "            labels = labels.to(device, dtype=torch.long)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            output = model(input_ids=input_ids,\n",
        "                           attention_mask=attention_mask,\n",
        "                           token_type_ids=token_type_ids,\n",
        "                           position_ids=position_ids,\n",
        "                           labels=labels)\n",
        "\n",
        "            loss = output.loss\n",
        "            loss.backward()\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            tepoch.set_postfix(loss=loss.item())\n",
        "            if iteration != 0 and iteration % int(len(train_loader) / 5) == 0:\n",
        "                # Evaluate the model five times per epoch\n",
        "                with torch.no_grad():\n",
        "                    model.eval()\n",
        "                    valid_losses = []\n",
        "                    predictions = []\n",
        "                    target_labels = []\n",
        "                    for input_ids, attention_mask, token_type_ids, position_ids, labels in tqdm(dev_loader,\n",
        "                                                                                                desc='Eval',\n",
        "                                                                                                position=1,\n",
        "                                                                                                leave=None):\n",
        "                        input_ids = input_ids.to(device)\n",
        "                        attention_mask = attention_mask.to(device)\n",
        "                        token_type_ids = token_type_ids.to(device)\n",
        "                        position_ids = position_ids.to(device)\n",
        "                        labels = labels.to(device, dtype=torch.long)\n",
        "\n",
        "                        output = model(input_ids=input_ids,\n",
        "                                       attention_mask=attention_mask,\n",
        "                                       token_type_ids=token_type_ids,\n",
        "                                       position_ids=position_ids,\n",
        "                                       labels=labels)\n",
        "\n",
        "                        logits = output.logits\n",
        "                        loss = output.loss\n",
        "                        valid_losses.append(loss.item())\n",
        "\n",
        "                        batch_predictions = [0 if example[0] > example[1] else 1 for example in logits]\n",
        "                        batch_labels = [int(example) for example in labels]\n",
        "\n",
        "                        predictions += batch_predictions\n",
        "                        target_labels += batch_labels\n",
        "\n",
        "                acc = compute_acc(predictions, target_labels)\n",
        "                valid_loss = sum(valid_losses) / len(valid_losses)\n",
        "                if lowest_valid_loss > valid_loss:\n",
        "                    print('Acc for model which have lower valid loss: ', acc)\n",
        "                    torch.save(model.state_dict(), \"./pytorch_model.bin\")"
      ],
      "metadata": {
        "id": "DuZfvzpGrvgW",
        "outputId": "221136c0-1620-422b-f766-203a271968ff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:  20%|█▉        | 346/1732 [01:19<05:07,  4.50batch/s, loss=0.0497]\n",
            "Eval:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Eval:  12%|█▎        | 1/8 [00:00<00:02,  3.42it/s]\u001b[A\n",
            "Eval:  25%|██▌       | 2/8 [00:00<00:01,  4.70it/s]\u001b[A\n",
            "Eval:  38%|███▊      | 3/8 [00:00<00:00,  5.30it/s]\u001b[A\n",
            "Eval:  50%|█████     | 4/8 [00:00<00:00,  5.82it/s]\u001b[A\n",
            "Eval:  62%|██████▎   | 5/8 [00:00<00:00,  5.72it/s]\u001b[A\n",
            "Eval:  75%|███████▌  | 6/8 [00:01<00:00,  5.80it/s]\u001b[A\n",
            "Eval:  88%|████████▊ | 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "Eval: 100%|██████████| 8/8 [00:01<00:00,  6.15it/s]\u001b[A\n",
            "                                                   \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc for model which have lower valid loss:  0.973\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:  40%|███▉      | 692/1732 [02:42<03:56,  4.40batch/s, loss=0.0491]\n",
            "Eval:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Eval:  12%|█▎        | 1/8 [00:00<00:01,  3.54it/s]\u001b[A\n",
            "Eval:  25%|██▌       | 2/8 [00:00<00:01,  4.83it/s]\u001b[A\n",
            "Eval:  38%|███▊      | 3/8 [00:00<00:00,  5.38it/s]\u001b[A\n",
            "Eval:  50%|█████     | 4/8 [00:00<00:00,  5.90it/s]\u001b[A\n",
            "Eval:  62%|██████▎   | 5/8 [00:00<00:00,  5.77it/s]\u001b[A\n",
            "Eval:  75%|███████▌  | 6/8 [00:01<00:00,  5.83it/s]\u001b[A\n",
            "Eval:  88%|████████▊ | 7/8 [00:01<00:00,  5.75it/s]\u001b[A\n",
            "Eval: 100%|██████████| 8/8 [00:01<00:00,  6.16it/s]\u001b[A\n",
            "                                                   \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc for model which have lower valid loss:  0.97675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:  60%|█████▉    | 1038/1732 [04:04<02:37,  4.39batch/s, loss=0.0391]\n",
            "Eval:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Eval:  12%|█▎        | 1/8 [00:00<00:02,  3.48it/s]\u001b[A\n",
            "Eval:  25%|██▌       | 2/8 [00:00<00:01,  4.74it/s]\u001b[A\n",
            "Eval:  38%|███▊      | 3/8 [00:00<00:00,  5.33it/s]\u001b[A\n",
            "Eval:  50%|█████     | 4/8 [00:00<00:00,  5.85it/s]\u001b[A\n",
            "Eval:  62%|██████▎   | 5/8 [00:00<00:00,  5.74it/s]\u001b[A\n",
            "Eval:  75%|███████▌  | 6/8 [00:01<00:00,  5.81it/s]\u001b[A\n",
            "Eval:  88%|████████▊ | 7/8 [00:01<00:00,  5.74it/s]\u001b[A\n",
            "Eval: 100%|██████████| 8/8 [00:01<00:00,  6.15it/s]\u001b[A\n",
            "                                                   \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc for model which have lower valid loss:  0.97825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0:  80%|███████▉  | 1384/1732 [05:26<01:20,  4.32batch/s, loss=0.0369]\n",
            "Eval:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Eval:  12%|█▎        | 1/8 [00:00<00:02,  3.48it/s]\u001b[A\n",
            "Eval:  25%|██▌       | 2/8 [00:00<00:01,  4.70it/s]\u001b[A\n",
            "Eval:  38%|███▊      | 3/8 [00:00<00:00,  5.28it/s]\u001b[A\n",
            "Eval:  50%|█████     | 4/8 [00:00<00:00,  5.82it/s]\u001b[A\n",
            "Eval:  62%|██████▎   | 5/8 [00:00<00:00,  5.72it/s]\u001b[A\n",
            "Eval:  75%|███████▌  | 6/8 [00:01<00:00,  5.80it/s]\u001b[A\n",
            "Eval:  88%|████████▊ | 7/8 [00:01<00:00,  5.73it/s]\u001b[A\n",
            "Eval: 100%|██████████| 8/8 [00:01<00:00,  6.14it/s]\u001b[A\n",
            "                                                   \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc for model which have lower valid loss:  0.97825\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|█████████▉| 1730/1732 [06:48<00:00,  4.18batch/s, loss=0.044] \n",
            "Eval:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
            "Eval:  12%|█▎        | 1/8 [00:00<00:02,  3.50it/s]\u001b[A\n",
            "Eval:  25%|██▌       | 2/8 [00:00<00:01,  4.78it/s]\u001b[A\n",
            "Eval:  38%|███▊      | 3/8 [00:00<00:00,  5.34it/s]\u001b[A\n",
            "Eval:  50%|█████     | 4/8 [00:00<00:00,  5.86it/s]\u001b[A\n",
            "Eval:  62%|██████▎   | 5/8 [00:00<00:00,  5.75it/s]\u001b[A\n",
            "Eval:  75%|███████▌  | 6/8 [00:01<00:00,  5.82it/s]\u001b[A\n",
            "Eval:  88%|████████▊ | 7/8 [00:01<00:00,  5.75it/s]\u001b[A\n",
            "Eval: 100%|██████████| 8/8 [00:01<00:00,  6.17it/s]\u001b[A\n",
            "                                                   \u001b[A"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc for model which have lower valid loss:  0.983\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 1732/1732 [06:50<00:00,  4.21batch/s, loss=0.00774]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "test_df = pd.read_csv('/content/drive/MyDrive/test_no_label.csv')"
      ],
      "metadata": {
        "id": "P95gtlnurvgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = test_df['Id']"
      ],
      "metadata": {
        "id": "cLDzC10ErvgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def make_id_file_test(tokenizer, test_dataset):\n",
        "    data_strings = []\n",
        "    id_file_data = [tokenizer.encode(sent.lower()) for sent in test_dataset]\n",
        "    for item in id_file_data:\n",
        "        data_strings.append(' '.join([str(k) for k in item]))\n",
        "    return data_strings"
      ],
      "metadata": {
        "id": "jnt693N0rvgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test = make_id_file_test(tokenizer, test_dataset)"
      ],
      "metadata": {
        "id": "7C5PpXtlrvgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test[:10]"
      ],
      "metadata": {
        "id": "1aqse7SHrvgY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9ca96444-ddcb-4af2-88f7-524f7f974afa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['101 2009 1005 1055 1037 2878 2047 3325 1998 2047 26389 2169 2051 2017 2175 1012 102',\n",
              " '101 2061 15640 2013 2019 2214 5440 1012 102',\n",
              " '101 2009 2003 1996 2087 14469 7273 1999 1996 3028 1012 102',\n",
              " '101 2079 2025 3696 1037 10084 2007 2122 2111 1012 102',\n",
              " '101 1045 2001 6091 1998 2016 2081 2033 2514 2061 6625 1998 6160 1012 102',\n",
              " '101 1996 2069 2518 2057 2363 2008 2001 2980 2001 1996 4157 1012 102',\n",
              " '101 2053 1010 2025 1996 3924 2012 2004 2226 1010 1996 3924 1999 3502 2152 1012 102',\n",
              " '101 2027 3288 2009 2041 2392 2005 2017 1998 2024 2200 14044 1012 102',\n",
              " '101 4606 1996 12043 2106 1050 1005 1056 2130 2113 2129 2000 2147 1996 3274 1012 102',\n",
              " '101 2027 2031 2019 6581 4989 1997 25025 2015 2000 5454 2013 1012 102']"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SentimentTestDataset(object):\n",
        "    def __init__(self, tokenizer, test):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = []\n",
        "\n",
        "        for sent in test:\n",
        "            self.data += [self._cast_to_int(sent.strip().split())]\n",
        "\n",
        "    def _cast_to_int(self, sample):\n",
        "        return [int(word_id) for word_id in sample]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sample = self.data[index]\n",
        "        return np.array(sample)"
      ],
      "metadata": {
        "id": "cZi14gnnrvgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataset = SentimentTestDataset(tokenizer, test)"
      ],
      "metadata": {
        "id": "erHjGE9rrvgY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn_style_test(samples):\n",
        "    input_ids = samples\n",
        "    max_len = max(len(input_id) for input_id in input_ids)\n",
        "    # sorted_indices = np.argsort([len(input_id) for input_id in input_ids])[::-1]\n",
        "    sorted_indices = [i for i in range(len(input_ids))]\n",
        "\n",
        "    input_ids = pad_sequence([torch.tensor(input_ids[index]) for index in sorted_indices],\n",
        "                             batch_first=True)\n",
        "    attention_mask = torch.tensor(\n",
        "        [[1] * len(input_ids[index]) + [0] * (max_len - len(input_ids[index])) for index in\n",
        "         sorted_indices])\n",
        "    token_type_ids = torch.tensor([[0] * len(input_ids[index]) for index in sorted_indices])\n",
        "    position_ids = torch.tensor([list(range(len(input_ids[index]))) for index in sorted_indices])\n",
        "\n",
        "    return input_ids, attention_mask, token_type_ids, position_ids"
      ],
      "metadata": {
        "id": "y03-nDX9rvgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_batch_size = 32\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=test_batch_size,\n",
        "                                          shuffle=False, collate_fn=collate_fn_style_test,\n",
        "                                          num_workers=2)"
      ],
      "metadata": {
        "id": "gZ0l1HparvgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    predictions = []\n",
        "    for input_ids, attention_mask, token_type_ids, position_ids in tqdm(test_loader,\n",
        "                                                                        desc='Test',\n",
        "                                                                        position=1,\n",
        "                                                                        leave=None):\n",
        "\n",
        "        input_ids = input_ids.to(device)\n",
        "        attention_mask = attention_mask.to(device)\n",
        "        token_type_ids = token_type_ids.to(device)\n",
        "        position_ids = position_ids.to(device)\n",
        "\n",
        "        output = model(input_ids=input_ids,\n",
        "                       attention_mask=attention_mask,\n",
        "                       token_type_ids=token_type_ids,\n",
        "                       position_ids=position_ids)\n",
        "\n",
        "        logits = output.logits\n",
        "        batch_predictions = [0 if example[0] > example[1] else 1 for example in logits]\n",
        "        predictions += batch_predictions"
      ],
      "metadata": {
        "id": "XoSHTbJUrvgZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb8eeb01-03aa-4d09-8656-eecc3c7a2fd6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "Test:   0%|          | 0/32 [00:00<?, ?it/s]\u001b[A\n",
            "Test:   6%|▋         | 2/32 [00:00<00:01, 17.61it/s]\u001b[A\n",
            "Test:  25%|██▌       | 8/32 [00:00<00:00, 40.27it/s]\u001b[A\n",
            "Test:  47%|████▋     | 15/32 [00:00<00:00, 50.85it/s]\u001b[A\n",
            "Test:  69%|██████▉   | 22/32 [00:00<00:00, 55.69it/s]\u001b[A\n",
            "Test:  91%|█████████ | 29/32 [00:00<00:00, 58.29it/s]\u001b[A\n",
            "                                                     \u001b[A"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_df['Category'] = predictions"
      ],
      "metadata": {
        "id": "tGO3aS-VrvgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_df.to_csv('submission.csv', index=False)"
      ],
      "metadata": {
        "id": "VndXxal3rvgZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}